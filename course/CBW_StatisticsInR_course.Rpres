Statistics in R
========================================================
author: MRC London Institute of Medical Sciences (http://bioinformatics.lms.mrc.ac.uk)
date: 25/Sep/2019
width: 1440
height: 1100
autosize: true
font-import: <link href='http://fonts.googleapis.com/css?family=Slabo+27px' rel='stylesheet' type='text/css'>
font-family: 'Slabo 27px', serif;
css:style.css

Outline
========================================================

- data summary

- hypothesis testing

- correlation, linear regression and ANOVA


Materials.
========================================================
id: materials

All prerequisites, links to material and slides for this course can be found on github.
* [LMS_StatisticsInR](https://lmsbioinformatics.github.io/LMS_StatisticsInR/)

Or can be downloaded as a zip archive from here.
* [Download zip](https://github.com/LMSBioinformatics/LMS_StatisticsInR/archive/master.zip)


Before we start...
========================================================
* Please make sure you activate the R script source panel
<div align="center">
<img src="figures/open_R_script.png" alt="path" height="700" width="1200">
</div>



Set the Working directory
========================================================

Before running any of the code in the practicals or slides we need to set the working directory to the folder we unarchived. 

You may navigate to the unarchived LMS_StatisticsInR/course folder in the Rstudio menu

**Session -> Set Working Directory -> Choose Directory**

<div align="center">
<img src="figures/R_set_path.png" alt="path" height="700" width="1200">
</div>


Set working directory - in the console
========================================================

Use getwd() to see where your current directory is

```{r,eval=F} 
getwd()
```

Use setwd() to set up your working directory

```{r,eval=F}
setwd("/Volumes/bioinfomatics$/yfwang/CBW/LMS_StatisticsInR/course")
# if you are working with Mac
# e.g. setwd("~/Downloads/LMS_StatisticsInR/course")

# if you are working with Windows
# e.g. setwd("~\\Downloads\\LMS_StatisticsInR\\course")

```

Materials. - Presentations, source code and practicals.
========================================================

Once the zip files are unzipped, all presentations are available as HTML slides and pages. Practical sheets will be available in the directories underneath.

* **exercises/**
Practicals as HTML pages.
* **answers/**
Practicals with answers as HTML pages.

Case study
========================================================
Start from some data for mouse strain BKS.Cg-Dock7m +/+ Leprdb/J (db/db)

Data Source: Jackson Laboratory (https://phenome.jax.org/projects/Jaxpheno17)

<img src="figures/good_format.png";>

Save excel file as csv (comma-separated values) file
========================================================
<img src="figures/save_as_CSV.png";>

We don't need a pretty excel file for the csv format
========================================================
<img src="figures/pretty_format.png";>

Load data
========================================================

```{r,prompt=F}
MouseData<-read.csv(file="data/mouse_BW_Fat_Glu_info.csv")
```

Data summary
========================================================

- data type

- spread of data

- shape of data

- distribution


Data type
========================================================

data type

- continuous

eg. blood pressure, body weight, height ...

- discrete

eg. gender, hair colour, RNA-seq read count...


Recap some basic R functions (1/7)
========================================================
- **head()**, **class()**, **str()**,  **dim()**, **colnames()** and **summary()** functions
- more details please see [Reproducible R course](https://lmsbioinformatics.github.io/LMS_Reproducible-R/)

-- [Data types in R]


Recap some basic R functions (2/7)
========================================================

**class()**: See the data type of a R object
```{r,prompt=F}
class(MouseData)
```

**head()**: See the first 6 lines of a R object
```{r,prompt=F}
head(MouseData)
```

Recap some basic R functions (3/7)
========================================================
see the how many rows and columns in "MouseData" object
```{r,prompt=F}
dim(MouseData)
```

see column names of "MouseData"
```{r,prompt=F}
colnames(MouseData)
```

Recap some basic R functions (4/7)
========================================================

**str()**: Compactly display the internal structure of an R object

Make sure the data format is correct for each column.
```{r,prompt=F}
str(MouseData)
```

Recap some basic R functions (5/7)
========================================================

```{r,prompt=F}
summary(MouseData)
```

Recap some basic R functions (6/7)
========================================================
**as.factor()** function: covert the data type of a vector to factor

```{r,prompt=F}
MouseData$Age<-as.factor(MouseData$Age)

summary(MouseData)
```

Recap some basic R functions (7/7)
========================================================

**ftable()**: Create ‘flat’ contingency tables
```{r}
ftable(MouseData[,c("Age","Genotype")])
```

Normal brackets **( )**  is for function

Square brackets **[ ]** is for subsetting

Spread of data - working with plots (boxplot)
========================================================
```{r,prompt=F,fig.height=10,fig.width=12}
library(ggplot2)
ggplot(MouseData, aes(x=Genotype, y=BW.gram, fill=Age)) + geom_boxplot()
```

Shape of data - working with plots 2 (Violin plot)
========================================================
```{r,prompt=F,fig.height=10,fig.width=12}
library(ggplot2)
ggplot(MouseData, aes(x=Genotype, y=BW.gram, fill=Age)) + 
  geom_violin(position=position_dodge(width = 0.5)) +
  geom_boxplot(width=.1, outlier.colour=NA,position=position_dodge(width = 0.5))

```


Spread of data - use body weight from WT mice (1/5)
========================================================

- working with plots
- more details please see [Reproducible R course](https://lmsbioinformatics.github.io/LMS_Reproducible-R/)

```{r,prompt=F}
head(MouseData)
```

```{r,prompt=F}
# subset the MouseData
WT_data<-MouseData[MouseData$Genotype=="+/+",]
WT_data$BW.gram
```

Spread of data - useful functions (2/5)
========================================================

**min()**, **max()**, **median()**, **range()** and **quantile()** functions
```{r,prompt=F}
# return the minimum
min.BW.gram<-min(WT_data$BW.gram)
# return the maximum
max.BW.gram<-max(WT_data$BW.gram)
# return the median
median.BW.gram<-median(WT_data$BW.gram)
# return the mean
mean.BW.gram<-mean(WT_data$BW.gram)
# show the value of variables above
c(min.BW.gram, max.BW.gram, median.BW.gram, mean.BW.gram)
range(WT_data$BW.gram)
quantile(WT_data$BW.gram)
```

Quick exercise - quantile()
========================================================

What will happen if you change the 
**probs** argument from the default settings to **seq(0, 1, 0.2)**?

```{r,prompt=F}
quantile(WT_data$BW.gram)
?quantile
```

## Usage

    quantile(x, probs = seq(0, 1, 0.25), na.rm = FALSE,
         names = TRUE, type = 7, ...)

Solution for the quick exercise - quantile()
========================================================

## Usage

    quantile(x, probs = seq(0, 1, 0.25), na.rm = FALSE,
         names = TRUE, type = 7, ...)
         
### when the name of the arguments are ignored, the order of arguments is important      
```{r,prompt=F}
quantile(WT_data$BW.gram, seq(0, 1, 0.2))
```

### when the name of the arguments are presented, the order of arguments is not important
```{r,eval=F,prompt=F}
# argument names are presented
quantile(x=WT_data$BW.gram, probs=seq(0, 1, 0.2))
# or part of the argument names are presented
quantile(x=WT_data$BW.gram, pro=seq(0, 1, 0.2))
# or
quantile(probs=seq(0, 1, 0.2), x=WT_data$BW.gram)
```

Spread of data - work with plots (3/5)
========================================================
Left: 40%

base R graphics boxplot - Spear

```{r, echo=FALSE,fig.width=3.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
boxplot(WT_data$BW.gram,ylab="Body Weight (gram)")
abline(h=mean(WT_data$BW.gram),col="forestgreen",lwd=3)
```
***
```{r,prompt=F}
c(min.BW.gram, max.BW.gram)
c(median.BW.gram, mean.BW.gram)
quantile(WT_data$BW.gram)
quantile(WT_data$BW.gram)[c(2,4)]

```

Spread of data -  work with plots (4/5)
========================================================
**summary()**
```{r,prompt=F}
summary(WT_data$BW.gram)
```
**range()**: show the minimum and maximum
```{r,prompt=F}
range(WT_data$BW.gram)
```
**IQR()**: show the interquartile range, i.e. 3rd quartile - 1st quartile
```{r,prompt=F}
IQR(WT_data$BW.gram)
```

Spread of data - work with plots (5/5)
========================================================

```{r, echo=FALSE,fig.width=3.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
library(ggplot2)
iqr15<-IQR(WT_data$BW.gram)*1.5
q1<-unname(quantile(WT_data$BW.gram)[2])
q3<-unname(quantile(WT_data$BW.gram)[4])
x<-WT_data$BW.gram
ggplot(WT_data, aes(x=Genotype, y=BW.gram))+
  geom_boxplot()#+
  #geom_hline(yintercept=c(min(max(x), q3+iqr15),
  #                        max(min(x), q1-iqr15)),
  #           col=c("red","blue"))

```

Tukey boxplot - geom_boxplot()

upper whisker = min(max(x), Q_3 + 1.5 * IQR) 

lower whisker = max(min(x), Q_1 – 1.5 * IQR)


Spread of data - working with plots (boxplot)
========================================================
```{r,echo=T,prompt=F,fig.height=10,fig.width=12}
library(ggplot2)
ggplot(MouseData, aes(x=Genotype, y=BW.gram, fill=Age)) + geom_boxplot()
```


Spread of data - more about boxplot (optional 1/3)
========================================================

Scatter plot: plot the WT mice's Body Weight against index

```{r,prompt=F,fig.width=3.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
plot(WT_data$BW.gram,ylab="Body Weight (gram)")
```

Spread of data - work with plots (optional 2/3)
========================================================
sort the data from min to max

```{r,prompt=F,echo=F,fig.width=3.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
plot(sort(WT_data$BW.gram),ylab="Body Weight (gram)")
```
***

start to see something here...

```{r,echo=F,fig.width=3.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
quan_1<-quantile(c(1:nrow(WT_data)))[2]
quan_3<-quantile(c(1:nrow(WT_data)))[4]
plot(sort(WT_data$BW.gram),ylab="Body Weight (gram)",xaxt="n")
  axis(side=1, at=c(0,quan_1,median(1:nrow(WT_data)),quan_3,nrow(WT_data)),
     labels=c("min","1stquantile","median","3rdquantile","max"))
  points(x=c(1,median(1:nrow(WT_data)),nrow(WT_data)),
         y=c(min(WT_data$BW.gram),
             median(WT_data$BW.gram),max(WT_data$BW.gram)),
         col="red",pch=16,cex=1.6)
  abline(h=quantile(WT_data$BW.gram)[c(2,4)],
         v=quantile(c(1:nrow(WT_data)))[c(2,4)],col="pink",lwd=3,lty=2)
```


Spread of data - work with plots (optional 3/3)
========================================================

```{r, echo=FALSE,fig.width=6,fig.height=6,dpi=300,out.width="750px",height="720px"}
par(mfrow=c(1,2))
  boxplot(WT_data$BW.gram,ylab="Body Weight (gram)")
  plot(sort(WT_data$BW.gram),ylab="Body Weight (gram)",xaxt="n")
  axis(side=1, at=c(0,quan_1,median(1:nrow(WT_data)),quan_3,nrow(WT_data)),
     labels=c("min","1stquantile","median","3rdquantile","max"))
  points(x=c(1,median(1:nrow(WT_data)),nrow(WT_data)),
         y=c(min(WT_data$BW.gram),
             median(WT_data$BW.gram),max(WT_data$BW.gram)),
         col="red",pch=16,cex=1.6)
  abline(h=quantile(WT_data$BW.gram)[c(2,4)],
         v=quantile(c(1:nrow(WT_data)))[c(2,4)],col="pink",lwd=3,lty=2)
par(mfrow=c(1,1))
```

Data shape - histogram (1/4)
========================================================

```{r,prompt=F,fig.width=4.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
hist(WT_data$BW.gram,breaks=10)
```

Data shape - histogram (2/4)
========================================================

```{r,prompt=F,fig.width=4.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
hist(WT_data$BW.gram,breaks=10,freq = F)
lines(density(WT_data$BW.gram),col="red")

```

Data shape - histogram (3/4)
========================================================

```{r,echo=F,out.width="850px",height="850px"}
par(mfrow=c(2,1))
  hist(WT_data$BW.gram,breaks=10,freq = F,
       xlim=range(WT_data$BW.gram),xlab="Body Weight (gram)")
  lines(density(WT_data$BW.gram),col="red",lwd=3)
  boxplot(WT_data$BW.gram,xlab="Body Weight (gram)",horizontal=T, ylim=range(WT_data$BW.gram))
  abline(v=mean(WT_data$BW.gram),col="forestgreen",lwd=3)
par(mfrow=c(1,1))
```


Data shape - violin plot (4/4)
========================================================
Left: 40%

```{r,echo=F}
par(mfrow=c(2,1))
  hist(WT_data$BW.gram,breaks=10,freq = F,xlab="Body Weight (gram)",
       xlim=range(WT_data$BW.gram))
  lines(density(WT_data$BW.gram),col="red",lwd=3)
  boxplot(WT_data$BW.gram,xlab="Body Weight (gram)",horizontal=T, ylim=range(WT_data$BW.gram))
  abline(v=mean(WT_data$BW.gram),col="forestgreen",lwd=3)
par(mfrow=c(1,1))
```
***

```{r,prompt=F, echo=F}
library(ggplot2)
ggplot(WT_data, aes(x=Genotype, y=BW.gram)) + 
  geom_violin(position=position_dodge(width = 0.5)) +
  ylim(range(WT_data$BW.gram))+xlab("")+
  geom_boxplot(width=.1, outlier.colour=NA,position=position_dodge(width = 0.5)) +
  theme_bw()+ coord_flip()

```

Spread of data - Variance and Standard deviation (1/3)
========================================================

$$
  \begin{aligned}
  \overline x  = \frac{{\displaystyle\sum_{i=1}^n}x_i}n \\
  \\ \\
  \text{Variance} = \sigma^2 = \frac{{\displaystyle\sum_{i=1}^n}(\left|x_i-\overline x\right|)^2}{n-1} \\
  \\ \\
  \text{Standard deviation} = \sigma =\sqrt{\text{Variance}} \\
  \\ \\
  \end{aligned}
$$
```{r,prompt=F}
# calculate variance
var.BW.gram<-sum((WT_data$BW.gram-mean.BW.gram)^2)/(nrow(WT_data)-1)
# calculate standard deviation
sd.BW.gram<-sqrt(var.BW.gram)
# show the results
c(var.BW.gram, sd.BW.gram)
```

Spread of data - var() and sd() function (2/3)
========================================================
```{r,prompt=F}
var(WT_data$BW.gram)
sd(WT_data$BW.gram)
```

More about SD and Variance (3/3)
========================================================
- we use the SD more often because it has the same units as the data BUT, if you know one, then you automatically know the other as well.

- in many analysis, variances are used more often, i.e. F-test


Time for an exercise!
========================================================

Exercise on this part can be found [here](./exercises/Session1_exercise1_part1.html)


Answers to exercise.
========================================================

Answers can be found [here](./answers/Session1_answers1_part1.html)




Distributions (1/10)
========================================================

- Binomial distribution
- Normal distribution

Binomial distribution (2/10)
========================================================
  
Example: flip a fair (50% of head and 50% of tail) coin 10 times

$$X \sim Binom(n,p)$$
  
$$
  n=\text{number of experiment}
\\
p=\text{probability of success}
$$
  
$$
  E(X)=np
\\
Var(X)=\frac{p(1-p)}n
\\
$$
  
In this case:
  
  $$X \sim B(10,0.5)$$
  
***
  
```{r,echo=F}
x <- seq(0, 100, by=1)
hx <- dbinom(x,100,0.2)
plot(x, hx, type="l", lty=2, lwd=2, col="brown4",
     xlab="x value",ylab="Density", main="Binominal distribution")

```

Normal distribution (3/10)
========================================================
  
Example: body temperature of 150 MRC LMS staff

$$X \sim Normal(\mu,\sigma^2)
\\
\mu=mean
\\
\sigma=\text{standard deviation}
$$
  
***
  
```{r,echo=F,fig.width=3.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
x <- seq(-4, 4, length=100)
hx <- dnorm(x)
plot(x, hx, type="l", lty=2, lwd=2, col="blue4",
     xlab="x value",ylab="Density", main="pdf (probability density function)\n of normal distribution",xaxt='n')
axis(side=1, at=c(-4,-2,0,2,4),
     labels=c(expression(paste(-2,sigma)),expression(-sigma),expression(mu),
              expression(sigma),expression(paste(2,sigma))))

```


Distributions (4/10)
========================================================
  R comes with functions for extracting information from most common distibutions types. An example of standard R functions for dealing with distibution can be seen here using the normal distributions.

- pnorm - cumulative distribution for x, aka c.d.f. (cumulative distribution function)
- qnorm - inverse of pnorm (from probability gives x)
- dnorm - distribution density
- rnorm - random number from normal distribution

Distributions (5/10)
========================================================
Similar functions are available for other distibution types including:
  
- pbinom (binomial)
- pt (T distribution)
- pnbinom (negative binomial),
- phyper (hypergeometric)


Normal distribution example (6/10)
========================================================
  
  We can use **rnorm()** function to generate random values following a normal distribution. Here we produce 10 normally distributed numeric values with mean 8 and standard deviation of 3


## Usage

    rnorm(n, mean = 0, sd = 1)

```{r}
set.seed(2)
rnorm(10,mean=8,sd=3)
```

If you want to regenerate the exact random numbers, use the **set.seed()** function before generating any random numbers.

```{r,echo=F}
set.seed(2)
data4plot<-rnorm(10,mean=8,sd=3)
data4plot_h<-dnorm(data4plot,mean=8,sd=3)
x <- seq(0,16,by=.1)
hx <- dnorm(x,mean=8,sd=3)
plot(x, hx, type="l", lty=2, lwd=2, col="blue4",
     xlab="x value",ylab="Density", main="Normal distribution",xaxt="n")
points(x=data4plot,y=data4plot_h,col="red",pch=16,cex=1.6)
axis(side=1, at=c(2,5,8,11,14),labels=c(2,5,8,11,14))
```


Normal distribution example (7/10)
========================================================
  We can also use these functions to interrogate values assuming a normal distribution for the data.

The probablity of a value being VERY close to 8 (or exactly 8 for discrete distributions) for a distribution of mean 8 and standard deviation 3.

## Usage

    dnorm(x, mean = 0, sd = 1, log = FALSE)
    
```{r}
dnorm(8,mean=8,sd=3)
```

```{r,echo=F}
set.seed(2)
data4plot<-rnorm(10,mean=8,sd=3)
maxdensity<-dnorm(8,mean=8,sd=3)
x <- seq(0,16,by=.1)
hx <- dnorm(x,mean=8,sd=3)
plot(x, hx, type="l", lty=2, lwd=2, col="blue4",ylim=c(0,maxdensity),
     xlab="x value",ylab="Density", main="Normal distribution",xaxt="n")
axis(side=1, at=c(2,5,8,11,14),labels=c(2,5,8,11,14))
abline(h=maxdensity,v=8,col="pink",lwd=3,lty=2)

```

Normal distribution example (8/10)
========================================================
  The probablity (P(X<=x)) of a value being less than 8 for a distribution of mean 8 and standard deviation 3.
```{r}
pnorm(8,mean=8,sd=3)
```

```{r,echo=F}
set.seed(2)
data4plot<-rnorm(10,mean=8,sd=3)
maxdensity<-dnorm(8,mean=8,sd=3)
curve(dnorm(x,8,3),xlim=c(0,16),main='Normal distribution',
      xlab="x value",ylab="Density",
      col="blue4",xaxt="n",type="l", lty=2, lwd=2)
cord.x <- c(0,seq(0,8,0.01),8)
cord.y <- c(0,dnorm(seq(0,8,0.01),mean=8,sd=3),0)
polygon(cord.x,cord.y,col='salmon')
axis(side=1, at=c(2,5,8,11,14),labels=c(2,5,8,11,14))

```

***
  
  The value for which i have a 50 percent being greater than given a normal distribution of mean 8 and standard deviation 3.

```{r}
qnorm(0.5,mean=8,sd=3)
```


Standard Normal distribution (9/10)
========================================================
  
  $$X \sim Normal(\mu,\sigma^2)$$
  
```{r,echo=F,fig.width=6,fig.height=5.5,dpi=300}
x <- seq(-4, 4, length=100)
hx <- dnorm(x)
plot(x, hx, type="l", lty=2, lwd=2, col="blue4",
     xlab="x value",ylab="Density", main="Normal distribution",xaxt='n')
axis(side=1, at=c(-4,-2,0,2,4),
     labels=c(expression(paste(-2,sigma)),expression(-sigma),expression(mu),
              expression(sigma),expression(paste(2,sigma))))

```
***
  
  Z-score transformation

$$
  Z=\frac{X-\mu}\sigma
\\
\\
Z \sim Normal(0,1)
$$
  
```{r,echo=F,fig.width=6,fig.height=5.5,dpi=300}
x <- seq(-4, 4, length=200)
hx <- dnorm(x)
plot(x, hx, type="l", lty=2, lwd=2, col="blue4",
     xlab="z score",ylab="Density", main="standardised Normal distribution",xaxt='n')
axis(side=1, at=c(-4,-2,0,2,4),
     labels=c(-2,-1,0,1,2))

```

Standard Normal distribution (10/10)
========================================================
  
```{r,fig.width=5,fig.height=4,dpi=300}
x<-rnorm(10000,mean=5, sd=2.5)
hist(x)

```
***
```{r,fig.width=5,fig.height=4,dpi=300}
ztransfer<-scale(x)
hist(ztransfer)
```



Time for an exercise!
========================================================

Exercise on this part can be found [here](./exercises/Session1_exercise1.html)


Answers to exercise.
========================================================

Answers can be found [here](./answers/Session1_answers1.html)



Hypothesis testing and ANOVA
========================================================

- SD (standard deviation) and SE (standard error; standard error of sample mean)

- Confidence Interval (CI)

- Hypothesis testing

 -- parametric test:e.g. t-test

 -- non-parametric test: e.g. Wilcoxon test; chi-square test and Fisher's exact test

- Analysis of Variance (ANOVA)


Statistical tests
========================================================

On top of descriptive statistics, R has several statistical tests covering a range of problems and data types.

Some common tests include:
- var.test() - Comparing 2 variances (Fisher's F test)
- t.test() - Comparing 2 sample means with normal errors (Student's t-test)
- wilcox.test() - Comparing 2 means with non-normal errors (Wilcoxon's rank test)
- fisher.test() - Testing for independence of 2 variables in a contingency table (Fisher's exact test)


Hypothesis testing for mean - t-test (1/8)
========================================================

**t.test()**

*one-sample t-test*
```{r,warning=FALSE, eval=F}
t.test(groupA,mu=something)
```
*independent t-test*

We are going to discuss this case here.
```{r,warning=FALSE, eval=F}
t.test(groupA,groupB,paired=FALSE)
```
*paired t-test*
```{r,warning=FALSE, eval=F}
t.test(Patients_before_treatment,Patients_after_treatment,paired=TRUE)
```

Hypothesis testing for mean - Load data (2/8)
========================================================

Use the WT dataset as example: Is the body weight of WT in Age 8 weeks and Age 16 weeks different. For the purpose of this session, let's assume the mouse body weight is normally distributed in WT 8 weeks and WT 16 weeks.


```{r, warning=FALSE, eval=T,echo=T}
WT_data<-MouseData[MouseData$Genotype=="+/+",]
boxplot(BW.gram~Age,data=WT_data)
```


Independent t-test example - Calculating variance (3/8)
========================================================
What is the difference in variances between WT Age 8 weeks and WT Age 16 weeks?

F test

$$F= \frac{S^2_x}{S^2_y}
\\
S^2_x:\text{ sample varience for group x}
\\
S^2_y:\text{ sample varience for gorup y}
\\\\
\text{degrees of freedom for the numerator}=n_x-1
\\
\text{degrees of freedom for the denominator}=n_y-1
$$


Calculating F test with R (4/8)
========================================================

We can test for any differences in variances between WT 8 weeks and WT 16 weeks with an F-test using the var.test() function.

$$H_0:\sigma_{WT_8w}^{2}= \sigma_{WT_16w}^{2}
\\
H_a:\sigma_{WT_8w}^{2}\neq \sigma_{WT_16w}^{2}$$

```{r}
WT_8w_data<-WT_data[WT_data$Age=="8" ,]
WT_16w_data<-WT_data[WT_data$Age=="16" ,]
var.test(WT_8w_data$BW.gram,WT_16w_data$BW.gram)

```

R objects (s3 and s4) (5/8)
========================================================
Left:30% The data type holding the result var.test() is a little more complex than the data types we have looked.

In R, special objects (S3 or S4 objects) can be created which have methods associated to them. The result from var.test is an object of class htest.

Since we have not come across this before, in order to discover its structure we can use the str() function with the object of interest as the argument.
```{r}
result <- var.test(WT_8w_data$BW.gram,WT_16w_data$BW.gram)
str(result)
```


R objects (s3 and s4) (6/8)
========================================================
Now we know the structure and class of the htest object we can access the slots containing information we want just as with a named list.

The p-value
```{r}
result$p.value
```
The statistic
```{r}
result$statistic
```
The data used in function call
```{r}
result$data.name
```

Independent t-test (7/8)
========================================================
We have ascertained that WT 8 weeks and WT 16 weeks have similar variances. We can therefore perform a standard t-test to assess the significance of differences between these groups.

$$H_0:\mu_{WT_{8w}}= \mu_{WT_{16w}}
\\
H_a:\mu_{WT_{8w}}\neq \mu_{WT_{16w}}$$

```{r}
test_res <- t.test(WT_8w_data$BW.gram,WT_16w_data$BW.gram,alternative ="two.sided", var.equal = T)
test_res

```

T-test example - Specifying a formula (8/8)
========================================================
The same result to that shown could be achieved by specifying a formula for the comparison. Here we wish to compare 8 weeks versus 16 weeks so we could simply specify the formula and the data to be used.

```{r}
result_formula <- t.test(BW.gram~Age,WT_data,alternative ="two.sided", var.equal = T)
result_formula
```

Non-parametric test
========================================================
  
  Non-parametric statistical hypothesis test is a test that is not based on probability distribution for the dependant variable. 

It doesn't repuire the dependent varible to be normally distributed.

**wilcox.test()**

Wilcoxon Signed-Rank Test is one of the Non-parametric statistical hypothesis tests. It is a good alternative to t-tests without assuming the dependent variables to follow the normal distribution.

t-test and Wilcoxon test alternatives
========================================================

**t.test()**

*one-sample t-test*
```{r,warning=FALSE, eval=F}
t.test(groupA,mu=something)
```
*independent t-test*
```{r,warning=FALSE, eval=F}
t.test(groupA,groupB,paired=FALSE)
```
*paired t-test*
```{r,warning=FALSE, eval=F}
t.test(groupA,groupB,paired=TRUE)
```
***
**wilcox.test()**

*one-sample Wilcoxon: Signed-Rank Test*
```{r,warning=FALSE, eval=F}
wilcox.test(groupA,mu=something)
```
*Wilcoxon Rank Sum Test: Mann-Whitney U*
```{r,warning=FALSE, eval=F}
wilcox.test(groupA,groupB,paired=FALSE)
```
*paired Wilcoxon:Signed-Rank Test*
```{r,warning=FALSE, eval=F}
wilcox.test(groupA,groupB,paired=TRUE)
```

Wilcoxon test - wilcox.test()
========================================================

Wilcoxon Signed-Rank Test is one of the Non-parametric statistical hypothesis tests. It is a good alternative to t-tests without assuming them to follow the normal distribution.

$$H_0: \text{median}_{a}- \text{median}_{b} = 0
\\
H_a: \text{median}_{a}- \text{median}_{b}\neq 0$$

Back to our mouse dataset
========================================================

Is there body weight different between WT and KO mice?

```{r}
WT_dbdb_data<-MouseData[MouseData$Genotype!="db/+",]
WT_dbdb_data<-droplevels(WT_dbdb_data)
boxplot(BW.gram~Genotype,data=WT_dbdb_data)
```


Wilcoxon test
========================================================
left: 70%
**qqnorm()** and **qqline()**

Check normal distribution with normal quantile plots for WT data

```{r}
WT4wilcox<-WT_dbdb_data[WT_dbdb_data$Genotype=="+/+",]

qqnorm(WT4wilcox$BW.gram)
qqline(WT4wilcox$BW.gram)

```

***

```{r}
shapiro.test(WT4wilcox$BW.gram)

```

Wilcoxon test
========================================================
left: 70%
**qqnorm()** and **qqline()**

Check normal distribution with normal quantile plots for db/db data

```{r}
KO4wilcox<-WT_dbdb_data[WT_dbdb_data$Genotype=="db/db",]

qqnorm(KO4wilcox$BW.gram)
qqline(KO4wilcox$BW.gram)

```

***

```{r}
shapiro.test(KO4wilcox$BW.gram)
```

Wilcoxon test (Mann-Whitney U)
========================================================
**wilcox.test()**

```{r}

wilcox.test(WT4wilcox$BW.gram,KO4wilcox$BW.gram, paired=F)

```

fisher.test()
========================================================

Given two gene lists, tests the significance of their overlap in comparison with a genomic background.

$$H_0:\text{ the odds ratio is no larger than 1}
\\
H_a:\text{ the odds ratio is larger than 1 }$$

Assuming there are 20,000 genes in the mouse genome, we have differential expression (DE) gene list A (300 genes) from a RNAseq experiment, and a gene list B (50 genes) relating to T cell migration from the literature. The number of overlap genes between DE genes and T cell migration genes is 5. Is the overlap between the two list significant?

fisher.test()
========================================================
create a contigency table

```{r,prompt=F}
cmatrix<-matrix(c(5,45,295,19655),byrow=T,ncol=2,dimnames=list(c("In.B","Not.In.B"),c("In.A","Not.In.A")))
cmatrix

```

fisher.test()
========================================================
**fisher.test()**

```{r}
fisher.test(cmatrix)
```


  
ANOVA (1/5)
========================================================
  
  Compute analysis of variance (or deviance), a.k.a. ANOVA, for one or more fitted model objects.

ANOVA is a statistical method that uses F-test to test

$$H_0:\mu_{1}= \mu_{2}=... \mu_{k}$$
  
  by comparing the variability between groups to the variability within groups
  
Assume that 

(1) all samples are independent and have >2 categorical groups; 

(2) dependent variable is continuous

(3) data of each group is normally distributed

(4) homogeneity of variances

ANOVA (2/5)
========================================================
```{r}
boxplot(BW.gram~Genotype,data=MouseData)
```


ANOVA - use the lm() function (3/5)
========================================================
```{r}
lmPG<-lm(formula = BW.gram ~ Genotype,data = MouseData)
lmPG

```

ANOVA - use the anova() function (4/5)
========================================================
```{r}
anova_PG<-anova(lmPG)
anova_PG
```

ANOVA - post-hoc analysis (5/5)
========================================================
**TukeyHSD** - Test which of the groups have different means

```{r}
TukeyHSD(aov(lmPG))
```


Time for an exercise!
========================================================

Exercise on this session can be found [here](exercises/Session2_exercise2.html)


Answers to exercise.
========================================================

Answers can be found [here](answers/Session2_answers2.html)


Correlation (1/6)
=========================================================

A common task in statistical analysis is to investigate the linear relationship between pairs of numeric vectors.

This can be done by identifying the correlation between numeric vectors using the **cor()** function in R.

In this example we use **cor()** to identify the Pearson correlation between two variables.  The **method** argument may be set to make use of different correlation methods.

- Perfectly posively correlated vectors will return 1
- Perfectly negatively correlated vectors will return -1
- Vectors showing no or little linear correlation will be close to 0.
- when it comes to correlation, always check the scatter plot for your data

Correlation between vectors (2/6)
=========================================================

```{r,prompt=F}
x <- rnorm(100,10,2)
z <- rnorm(100,10,2)
y <- x
cor(x,y) #
cor(x,-y)
cor(x,z)
```
***
```{r,echo=F,prompt=F}
par(mfrow=c(3,1))
plot(x,y) #
plot(x,-y)
plot(x,z)

par(mfrow=c(1,1))
```

Correlation example (3/6)
=========================================================

Example of our mouse data. We would like to see whether there is a relationship between body weight and the percentage of fat tissue in the WT mice.

```{r,prompt=F}
KO_data<-MouseData[MouseData$Genotype=="db/db",]
head(KO_data)
cor(KO_data$BW.gram,KO_data$FatTissue.percent)

```
***
```{r,echo=F,prompt=F,fig.width=6,fig.height=5.5,dpi=300}
library("ggplot2")
p <- ggplot(KO_data, aes(BW.gram, FatTissue.percent))
p + geom_point()+
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14,face="bold"))

```

Correlation over a matrix (4/6)
=========================================================
left: 70%
Often we wish to apply correlation analysis to all columns or rows in a matrix in a pair-wise manner. To do this in R, we can simply pass the **cor()** function a single argument of the numeric matrix of interest. The **cor()** function will then perform all pair-wise correlations between columns.

- subset mouse dataset
```{r,prompt=F}
head(KO_data)
mouse4cor<-KO_data[,c(6:8)]; 

```

Correlation over a matrix (5/6)
=========================================================
```{r,prompt=F}
cor(mouse4cor)
```
```{r,eval=T,echo=F,fig.width=4,fig.height=2.5,dpi=300}
image(cor(mouse4cor),axes=F)
mtext(colnames(mouse4cor),side=2,at=seq(0,1,length.out=3),las=1,cex=0.6)
mtext(colnames(mouse4cor),side=3,at=seq(0,1,length.out=3),las=2,cex=0.6)

```

Correlation (6/6)
========================================================
```{r,prompt=F,fig.width=5,fig.height=5,dpi=300,out.width="650px",height="650px"}
pairs(mouse4cor)
```

Quick practice
========================================================
```{r,prompt=F,fig.width=5,fig.height=5,dpi=300,out.width="650px",height="650px"}
cor_aq<-read.csv("data/AQ.csv")
head(cor_aq)
```
Please calculate the following correlations
  
  x1 vs y1
  
  x2 vs y2
  
  x3 vs y3
  
  x4 vs y4

Solution for quick practice
========================================================
```{r,prompt=F,fig.width=5,fig.height=5,dpi=300,out.width="650px",height="650px"}
c1<- cor(cor_aq$x1, cor_aq$y1)
c2<- cor(cor_aq$x2, cor_aq$y2)
c3<- cor(cor_aq$x3, cor_aq$y3)
c4<- cor(cor_aq$x4, cor_aq$y4)
c(c1, c2, c3, c4)
```


Scatter plot for the quick practice - Anscombe's quartet
========================================================
```{r,echo=F,prompt=F,fig.width=10,fig.height=10,dpi=300,out.width="1000px",height="800px"}
par(mfrow=c(2,2))
plot(cor_aq$x1, cor_aq$y1, pch=20, col="blue", cex=2)
plot(cor_aq$x2, cor_aq$y2, pch=20, col="blue", cex=2)
plot(cor_aq$x3, cor_aq$y3, pch=20, col="blue", cex=2)
plot(cor_aq$x4, cor_aq$y4, pch=20, col="blue", cex=2)
par(mfrow=c(1,1))
```

Linear regression (1/23)
=========================================================

We have seen how we can find the linear correlation between two sets of variables using **cor()** function.

R also provides a comprehensive set of tools for regression analysis including the well used linear modeling function **lm()**

- least square method

*minimize the vertical distance between the fitted line and data points* 

```{r,echo=F,prompt=F,fig.width=3,fig.height=3,dpi=300,out.width="650px",height="650px"}
library("ggplot2")

pwh <-  ggplot(KO_data, aes(FatTissue.percent, BW.gram))
pwh + geom_point() + labs(x = "Body weight (gram)",y="Fat Tissue (%)") + 
   stat_smooth(method="lm",se=T)

```


Linear regression (2/23)
=========================================================
left: 70%
We use KO mouse dataset as example and see whether we can use mouse body weight to predict the percentage of fat tissue.
```{r,prompt=F}
KO_data<-MouseData[MouseData$Genotype=="db/db",]
head(KO_data)

```
***
```{r,echo=F,prompt=F,fig.width=4,fig.height=4,dpi=300,out.width="820px",height="820px"}
pwh <-  ggplot(KO_data, aes(BW.gram, FatTissue.percent))
pwh + geom_point() + labs(x = "Body weight (gram)",y="Fat Tissue (%)")
```


Linear regression (3/23)
=========================================================
The **lm()** function fits a linear regression to your data and provides useful information on the generated fit.

In the example below we fit a linear model using  **lm()** on the *KO_data* dataset with *FatTissue.percent* (Y) as the dependent variable and *BW.gram* (X) as the explanatory variable.
```{r,prompt=F}
lmResult<-lm(formula = FatTissue.percent ~ BW.gram, data = KO_data)
lmResult
```


Interpret output of lm() (4/23)
=========================================================

As we have seen, printing the model result provides the intercept and slope of line.
To get some more information on the model we can use the **summary()** function
```{r,prompt=F}
summary(lmResult)
```

Interpret output of lm() - coefficients (5/23)
=========================================================
left: 70%
```{r,prompt=F}
lmResult$coefficients
```
From the **$coefficients** of object *lmResult*, we know the equation for the best fit is

**$$Y = 36.6140076 + 0.5026053 *X$$**

**$$f(x)  = b_0 + b_1x$$**

$$b_0\text{: the value of f(x) when x =0}$$

```{r}
# the Intercept 36.6140076 is the expected percentage of fat tissue of a 0 body weight
# not interesting to any biological questions
```

$$b_1\text{: the amount of f(x) will change when x changes 1 unit}$$

```{r}
# For every gram increased in the mice body weight, we expect 0.50 (%) increased in the Fat tissue

```
***
```{r,echo=F,prompt=F,fig.width=3.5,fig.height=3.5,dpi=300,out.width="720px",height="720px"}
pwh <-  ggplot(KO_data, aes(BW.gram, FatTissue.percent))
pwh + geom_point() + stat_smooth(method = "lm",se=F) +
  labs(x = "Body weight (gram)",y="Fat Tissue (%)")
```


More about coefficients (6/23)
=========================================================

Predict the percentage of fat tissue with the body weight information.

If we have 3 KO mice with weight = 40, 55 and 66 grams, how do we predict their percentage of fat tissue?


Use the information from the *$coefficients*
```{r,prompt=F}
new_mouse_bw<-c(40,55,66)
beta0<-lmResult$coefficients[1]
beta1<-lmResult$coefficients[2]

predicted_new_fat<-beta0+beta1*new_mouse_bw
predicted_new_fat
```

Or use the *predict()*
```{r}
new_mouse_bw_df <- data.frame(BW.gram=c(40,55,66))
cleaver_predicted_fat<-predict(lmResult,new_mouse_bw_df)
cleaver_predicted_fat

```



Interpret output of lm() - residuals (7/23)
=========================================================

The **residuals** are the difference between the predicted and actual values.
To retrieve the residuals we can access the slot or use the **resid()** function.

```{r,prompt=F,echo=T}
summary(resid(lmResult))
summary(lmResult$residual)
```
Ideally you would want your residuals to be normally distributed around 0.

$$
E[e_{i}]=0
$$

More about residuals (8/23)
=========================================================

Plot the residuals

```{r,echo=T,fig.width=9,fig.height=7,dpi=300,out.width="1000",height="750"}
plot(KO_data$BW.gram,KO_data$FatTissue.percent,ylim=c(50,75),
     ylab="Fat tissue (%)",xlab="body weight (gram)")
abline(lmResult,col="blueviolet",lwd=3, lty=1)

```

More about residuals (9/23)
=========================================================

Residual is the vertical distance between the observed data and the regression line. It has the same unit as the dependent variable.

```{r,echo=F,fig.width=9,fig.height=7,dpi=300,out.width="1020",height="800"}
plot(KO_data$BW.gram,KO_data$FatTissue.percent,ylim=c(50,75),
     ylab="Fat tissue (%)",xlab="body weight (gram)")
abline(lmResult,col="blueviolet",lwd=3, lty=1)
y<-KO_data$FatTissue.percent;x<-KO_data$BW.gram
yhat<-predict(lmResult)
for (i in 1:nrow(KO_data)){
  lines(c(x[i],x[i]),
        c(y[i],yhat[i]),
        col="red",lwd=2)
}
```

More about residuals (10/23)
=========================================================

SSE shows the residual variability

It shows the variability that cannot be explained by the regression model

```{r,echo=F,fig.width=9,fig.height=7,dpi=300,out.width="1020",height="800"}
plot(KO_data$BW.gram,KO_data$FatTissue.percent,ylim=c(50,75),
     ylab="Fat tissue (%)",xlab="body weight (gram)")
abline(lmResult,col="blueviolet",lwd=3, lty=1)
y<-KO_data$FatTissue.percent;x<-KO_data$BW.gram
yhat<-predict(lmResult)
for (i in 1:nrow(KO_data)){
  lines(c(x[i],x[i]),
        c(y[i],yhat[i]),
        col="red",lwd=2)
}
```
***
$$Error_i = y_i - \hat{y_i} $$

$$y_i\text{: the observed percentage of fat tissue of ith mouse}$$

$$\hat{y_i}\text{: the predicted percentage of fat tissue of ith mouse}$$

$$Error_i^2  = (y_i - \hat{y_i})^2$$

$$\text{- sum of the square of the residuals (SSE)}$$

$$SSE  = \sum_{i=1}^{n}(y_i-\hat{y_i})^2$$



More about residuals (11/23)
=========================================================

Plot the residuals against the independent variable (X), i.e. the body weight It makes the residual accessment easiler by eyes.

```{r,echo=T,fig.width=9,fig.height=7,dpi=300,out.width="1000px",height="750px"}
plot(KO_data$BW.gram,lmResult$residual,ylim=c(-10,10),
     ylab="residuals (fat tissue %)",xlab="body weight (gram)")
abline(h=0,col="blueviolet",lwd=3, lty=1)

```

More about residuals (12/23)
=========================================================

Plot the residuals against the independent variable (X), i.e. the body weight. 

```{r,echo=F,fig.width=9,fig.height=7,dpi=300,out.width="1020",height="800px"}
plot(KO_data$BW.gram,lmResult$residual,ylim=c(-10,10),
     ylab="residuals (fat tissue %)",xlab="body weight (gram)")
abline(h=0,col="blueviolet",lwd=3, lty=1)
x<-KO_data$BW.gram
resid<-lmResult$residuals
for (i in 1:nrow(KO_data)){
  lines(c(x[i],x[i]),c(0,resid[i]), col="red",lwd=2)
}
```

More about residuals (13/23)
=========================================================

Plot the residuals against the independent variable (X)

```{r,echo=F,fig.width=5,fig.height=5,dpi=300,out.width="720px",height="720px"}
plot(KO_data$BW.gram,lmResult$residual,ylim=c(-10,10),
     ylab="residuals (fat tissue %)",xlab="body weight (gram)")
abline(h=0,col="blueviolet",lwd=3, lty=1)
x<-KO_data$BW.gram
resid<-lmResult$residuals
for (i in 1:nrow(KO_data)){
  lines(c(x[i],x[i]),c(0,resid[i]), col="red",lwd=2)
}

```

***
$$
Error_i = y_i - \hat{y_i}
\\

Error_i^2  = (y_i - \hat{y_i})^2
\\
\text{- sum of the square of the residuals (SSE)}
\\
SSE  = \sum_{i=1}^{n}(y_i-\hat{y_i})^2
$$


Interpret output of lm() - R-squared (14/23)
=========================================================

- The **R-squared** value represents the proportion of variability in the response variable that is explained by the explanatory variable.

- A high **R-squared** here indicates that the line fits closely to the data.

```{r,prompt=F,echo=T}
summary(lmResult)$r.squared
```


More about R-squared (15/23)
=========================================================

- Question: How would you describe (or summarize) the percentage of fat tissue for KO mice when the **body weight information is absence**? Which information you would use to predict a new KO mouse's percentage of fat tissue?

```{r,prompt=F,echo=T}
KO_data$FatTissue.percent
```

More about R-squared (16/23)
=========================================================

- Question: How would you describe (or summarize) the percentage of fat tissue for KO mice when the **body weight information is absence**? Which information you would use to predict a new KO mouse's percentage of fat tissue?

- mean might be a good choice

```{r,prompt=F,echo=T}
mean(KO_data$FatTissue.percent)
```

- If we have a new mouse, we could assume that the new KO mouse's percentage of fat tissue is around 60.365 %

More about R-squared (17/23)
=========================================================

- Question: How would you describe (or summarize) the percentage of fat tissue for KO mice when the **body weight information is absence**? Which information you would use to predict a new KO mouse's percentage of fat tissue?

- mean might be a good choice

```{r,echo=F,fig.width=5,fig.height=5,dpi=300,out.width="720px",height="720px"}
diff_df<-KO_data$FatTissue.percent-mean(KO_data$FatTissue.percent)
plot(KO_data$FatTissue.percent, ylim=c(-5,80),
     ylab="percentage of fat tissue", xlab="x")
abline(h=mean(KO_data$FatTissue.percent),
       col="forestgreen",lwd=3,lty=1)
```

More about R-squared - TSS (18/23)
=========================================================

```{r,echo=F,fig.width=5,fig.height=5,dpi=300,out.width="720px",height="720px"}
diff_df<-KO_data$FatTissue.percent-mean(KO_data$FatTissue.percent)
plot(KO_data$FatTissue.percent, ylim=c(-5,80),
     ylab="fat tissue (%)", xlab="x")
abline(h=mean(KO_data$FatTissue.percent),
       col="forestgreen",lwd=3,lty=1)
segments(x0=c(1:nrow(KO_data)),y0=KO_data$FatTissue.percent,
         x1=c(1:nrow(KO_data)),y1=mean(KO_data$FatTissue.percent),col="red",lwd=2)
```
***
Residuals from the mean: assuming the independent variable (X), i.e. body weight in our case, does not exist

$$
TSS=\text{Total Sum of Squares}=\sum_{i=1}^n(y_i-\overline y)^2
$$


More about  about R-squared (19/23)
=========================================================

Residuals from the mean: assuming the independent variable (X), i.e. body weight in our case, does not exist

```{r,echo=F}
diff_df<-KO_data$FatTissue.percent-mean(KO_data$FatTissue.percent)
plot(KO_data$FatTissue.percent, ylim=c(-5,80),
     ylab="rediduals (fat tissue %)", xlab="body weight (gram)")
abline(h=mean(KO_data$FatTissue.percent),
       col="forestgreen",lwd=3,lty=1)

segments(x0=c(1:nrow(KO_data)),y0=KO_data$FatTissue.percent,
         x1=c(1:nrow(KO_data)),y1=mean(KO_data$FatTissue.percent),col="red",lwd=2)
```

- Total Sum of Squares (TSS)

$$
  \begin{aligned}
  TSS  = \sum_{i=1}^{n}(y_i-\overline y)^2
  \end{aligned}
$$

***

Residuals from the model

```{r,echo=F}
plot(KO_data$BW.gram,lmResult$residual,ylim=c(-5,80),
     ylab="rediduals (fat tissue %)",xlab="body weight (gram)")
abline(h=0,col="blueviolet",lwd=3, lty=1)
x<-KO_data$BW.gram
resid<-lmResult$residuals
for (i in 1:nrow(KO_data)){
  lines(c(x[i],x[i]),c(0,resid[i]), col="red",lwd=2)
}
```
- Sum of the square of the residuals (SSE)
$$
SSE  = \sum_{i=1}^{n}(y_i-\hat{y_i})^2
$$

More about R-squared (20/23)
=========================================================

```{r,warning=F,echo=F,fig.width=6,fig.height=6,dpi=300,out.width="720px",height="720px"}

library(ggplot2,quietly=T)
e<-c(resid(lm(FatTissue.percent~1,data=KO_data)),
     resid(lm(FatTissue.percent~BW.gram,data=KO_data)))
fit4figure<-factor(c(rep("Fit2mean",nrow(KO_data)),
                   rep("Fit2BW",nrow(KO_data))), levels=c("Fit2mean","Fit2BW"))
ggplot(data.frame(e=e,fit=fit4figure),aes(y=e,x=fit,fill=fit)) + 
  geom_dotplot(binaxis="y", stackdir="center",binwidth = 1) + xlab("Model fit") +
  ylab("residuals (fat tissue %)")

```

More about R-squared - Calculating R-squared (21/23)
=========================================================

The fraction of variability in the independent variable (Y; or the *percentage of fat tissue* in this example) that can be explained by the explanatory variable (X; or the *body weight* in this example).

$$
TSS=\text{Total Sum of Squares}=\sum_{i=1}^n(y_i-\overline y)^2
\\
SSE=\text{Sum of the Square of the residuals}=\sum_{i=1}^n(y_i-\hat{y})^2
$$

```{r,prompt=F}
SSE<-sum(resid(lm(FatTissue.percent~BW.gram,data=KO_data))^2)
TSS<-sum(resid(lm(FatTissue.percent~1,data=KO_data))^2)
R_square<-1-(SSE/TSS)
R_square
summary(lmResult)$r.squared
```


Interpret output of lm() - F-statistics (22/23)
=========================================================

The R-squared shows the fraction of the total variability that is explained by the linear relationship with the explanatory variable. However, it does not provide a formal hypothesis test for this relationship. 

The F-test results from linear models also provides a measure of significance for a variable not being relevant

```{r,prompt=F,echo=T}
summary(lmResult)$fstatistic
```

More about F-statistics - Calculating F-stat (23/23)
=========================================================

$$
F=\frac{MSM}{MSE}=\frac{\text{mean of the explained variance}}{\text{mean of the unexplained variance}}=\frac{({\displaystyle\frac{SSM}1})}{({\displaystyle\frac{SSE}{n-2}})}
$$

```{r,prompt=F}

n=nrow(KO_data)
SSM <- sum((predict(lmResult) - mean(KO_data$FatTissue.percent))^2)
MSE <-sum(lmResult$residuals^2)/(n-2)

MSM <-SSM/1

MSM/MSE

summary(lmResult)$fstatistic
```

Time for an exercise!
========================================================

Exercise on this session can be found [here](exercises/Session3_exercise3.html)



Answers to exercise.
========================================================

Answers can be found [here](answers/Session3_answers3.html)


